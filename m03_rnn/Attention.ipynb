{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d721590b-b80e-408d-8c64-71189248e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7add838-ba6b-4d33-8e74-2381e203fc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mscore_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dot'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Dot-product attention layer, a.k.a. Luong-style attention.\n",
      "\n",
      "Inputs are a list with 2 or 3 elements:\n",
      "1. A `query` tensor of shape `(batch_size, Tq, dim)`.\n",
      "2. A `value` tensor of shape `(batch_size, Tv, dim)`.\n",
      "3. A optional `key` tensor of shape `(batch_size, Tv, dim)`. If none\n",
      "    supplied, `value` will be used as a `key`.\n",
      "\n",
      "The calculation follows the steps:\n",
      "1. Calculate attention scores using `query` and `key` with shape\n",
      "    `(batch_size, Tq, Tv)`.\n",
      "2. Use scores to calculate a softmax distribution with shape\n",
      "    `(batch_size, Tq, Tv)`.\n",
      "3. Use the softmax distribution to create a linear combination of `value`\n",
      "    with shape `(batch_size, Tq, dim)`.\n",
      "\n",
      "Args:\n",
      "    use_scale: If `True`, will create a scalar variable to scale the\n",
      "        attention scores.\n",
      "    dropout: Float between 0 and 1. Fraction of the units to drop for the\n",
      "        attention scores. Defaults to `0.0`.\n",
      "    seed: A Python integer to use as random seed incase of `dropout`.\n",
      "    score_mode: Function to use to compute attention scores, one of\n",
      "        `{\"dot\", \"concat\"}`. `\"dot\"` refers to the dot product between the\n",
      "        query and key vectors. `\"concat\"` refers to the hyperbolic tangent\n",
      "        of the concatenation of the `query` and `key` vectors.\n",
      "\n",
      "Call arguments:\n",
      "    inputs: List of the following tensors:\n",
      "        - `query`: Query tensor of shape `(batch_size, Tq, dim)`.\n",
      "        - `value`: Value tensor of shape `(batch_size, Tv, dim)`.\n",
      "        - `key`: Optional key tensor of shape `(batch_size, Tv, dim)`. If\n",
      "            not given, will use `value` for both `key` and `value`, which is\n",
      "            the most common case.\n",
      "    mask: List of the following tensors:\n",
      "        - `query_mask`: A boolean mask tensor of shape `(batch_size, Tq)`.\n",
      "            If given, the output will be zero at the positions where\n",
      "            `mask==False`.\n",
      "        - `value_mask`: A boolean mask tensor of shape `(batch_size, Tv)`.\n",
      "            If given, will apply the mask such that values at positions\n",
      "             where `mask==False` do not contribute to the result.\n",
      "    return_attention_scores: bool, it `True`, returns the attention scores\n",
      "        (after masking and softmax) as an additional output argument.\n",
      "    training: Python boolean indicating whether the layer should behave in\n",
      "        training mode (adding dropout) or in inference mode (no dropout).\n",
      "    use_causal_mask: Boolean. Set to `True` for decoder self-attention. Adds\n",
      "        a mask such that position `i` cannot attend to positions `j > i`.\n",
      "        This prevents the flow of information from the future towards the\n",
      "        past. Defaults to `False`.\n",
      "\n",
      "Output:\n",
      "    Attention outputs of shape `(batch_size, Tq, dim)`.\n",
      "    (Optional) Attention scores after masking and softmax with shape\n",
      "        `(batch_size, Tq, Tv)`.\n",
      "\u001b[0;31mFile:\u001b[0m           /app/conda/envs/collegium/lib/python3.10/site-packages/keras/src/layers/attention/attention.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     AdditiveAttention"
     ]
    }
   ],
   "source": [
    "?Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda55a8a-16cd-4cea-8c25-b473ca307787",
   "metadata": {},
   "source": [
    "### Cross-Attention in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fad0140-e165-4580-a0aa-09915e574724",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32bfe9e0-5d37-47ad-9814-001e73b523d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728170802.137145   86957 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728170802.190515   86957 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728170802.191631   86957 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728170802.194559   86957 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728170802.195618   86957 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728170802.196608   86957 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728170802.372290   86957 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728170802.373431   86957 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1728170802.374421   86957 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tf.constant([\n",
    "    [\n",
    "        [1, 2],\n",
    "        [3, 4],\n",
    "    ]\n",
    "], dtype=tf.float32)\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "411d7ff7-8f7c-49f5-9a19-b56c525dc033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = tf.constant([\n",
    "    [\n",
    "        [1, 0],\n",
    "        [0, 1],\n",
    "    ]\n",
    "], dtype=tf.float32)\n",
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78eae2de-9540-407b-beba-1cc676e6783c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 4, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = tf.constant([\n",
    "    [\n",
    "        [1, 0],\n",
    "        [0, 1],\n",
    "        [1, 1],\n",
    "        [0, 0]\n",
    "    ]\n",
    "], dtype=tf.float32)\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb886632-d127-45a3-8a9d-6289d390a1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 4, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = a([q, v, k])\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b15fe2b5-ae83-414f-b938-06d776083edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.5378828, 2.5378828],\n",
       "       [2.462117 , 3.462117 ],\n",
       "       [2.       , 3.       ],\n",
       "       [2.       , 3.       ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70572f30-c523-4a9d-a3f1-108ccacbea50",
   "metadata": {},
   "source": [
    "### Cross-Attention in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6843fce9-3189-4a2a-9241-877b283c8088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.5378828, 2.5378828],\n",
       "        [2.462117 , 3.462117 ],\n",
       "        [2.       , 3.       ],\n",
       "        [2.       , 3.       ]]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_logit = [qm @ tf.transpose(km) for qm, km in zip(q, k)]\n",
    "similar_weight = tf.math.softmax(similar_logit)\n",
    "out = similar_weight @ v\n",
    "out.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef38ba2e-6077-4831-b484-def7c37929eb",
   "metadata": {},
   "source": [
    "### Self-Attention in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7b1eb47-c41e-4485-b905-1e558c96cf56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.73105854, 0.26894143],\n",
       "        [0.26894143, 0.73105854]]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a([k, k, k]).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58820d7-6b6f-475d-8099-40890e075e71",
   "metadata": {},
   "source": [
    "### Self-Attention in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fe8b9bb-ea34-43de-94de-deee8e126a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.73105854, 0.26894143],\n",
       "        [0.26894143, 0.73105854]]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_logit = [km @ tf.transpose(km) for km, km in zip(k, k)]\n",
    "similar_weight = tf.math.softmax(similar_logit)\n",
    "out = similar_weight @ k\n",
    "out.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1d4209-6578-4f81-b028-c0181f627caa",
   "metadata": {},
   "source": [
    "### Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ca99922-81ab-4fd1-aba3-f3e7378b891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m\n",
      "\u001b[0mMultiHeadAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mkey_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mvalue_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0muse_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mattention_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'glorot_uniform'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbias_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zeros'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbias_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mkernel_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "MultiHeadAttention layer.\n",
      "\n",
      "This is an implementation of multi-headed attention as described in the\n",
      "paper \"Attention is all you Need\"\n",
      "[Vaswani et al., 2017](https://arxiv.org/abs/1706.03762).\n",
      "If `query`, `key,` `value` are the same, then\n",
      "this is self-attention. Each timestep in `query` attends to the\n",
      "corresponding sequence in `key`, and returns a fixed-width vector.\n",
      "\n",
      "This layer first projects `query`, `key` and `value`. These are\n",
      "(effectively) a list of tensors of length `num_attention_heads`, where the\n",
      "corresponding shapes are `(batch_size, <query dimensions>, key_dim)`,\n",
      "`(batch_size, <key/value dimensions>, key_dim)`,\n",
      "`(batch_size, <key/value dimensions>, value_dim)`.\n",
      "\n",
      "Then, the query and key tensors are dot-producted and scaled. These are\n",
      "softmaxed to obtain attention probabilities. The value tensors are then\n",
      "interpolated by these probabilities, then concatenated back to a single\n",
      "tensor.\n",
      "\n",
      "Finally, the result tensor with the last dimension as `value_dim` can take\n",
      "a linear projection and return.\n",
      "\n",
      "Args:\n",
      "    num_heads: Number of attention heads.\n",
      "    key_dim: Size of each attention head for query and key.\n",
      "    value_dim: Size of each attention head for value.\n",
      "    dropout: Dropout probability.\n",
      "    use_bias: Boolean, whether the dense layers use bias vectors/matrices.\n",
      "    output_shape: The expected shape of an output tensor, besides the batch\n",
      "        and sequence dims. If not specified, projects back to the query\n",
      "        feature dim (the query input's last dimension).\n",
      "    attention_axes: axes over which the attention is applied. `None` means\n",
      "        attention over all axes, but batch, heads, and features.\n",
      "    kernel_initializer: Initializer for dense layer kernels.\n",
      "    bias_initializer: Initializer for dense layer biases.\n",
      "    kernel_regularizer: Regularizer for dense layer kernels.\n",
      "    bias_regularizer: Regularizer for dense layer biases.\n",
      "    activity_regularizer: Regularizer for dense layer activity.\n",
      "    kernel_constraint: Constraint for dense layer kernels.\n",
      "    bias_constraint: Constraint for dense layer kernels.\n",
      "    seed: Optional integer to seed the dropout layer.\n",
      "\n",
      "Call arguments:\n",
      "    query: Query tensor of shape `(B, T, dim)`, where `B` is the batch size,\n",
      "        `T` is the target sequence length, and dim is the feature dimension.\n",
      "    value: Value tensor of shape `(B, S, dim)`, where `B` is the batch size,\n",
      "        `S` is the source sequence length, and dim is the feature dimension.\n",
      "    key: Optional key tensor of shape `(B, S, dim)`. If not given, will\n",
      "        use `value` for both `key` and `value`, which is the most common\n",
      "        case.\n",
      "    attention_mask: a boolean mask of shape `(B, T, S)`, that prevents\n",
      "        attention to certain positions. The boolean mask specifies which\n",
      "        query elements can attend to which key elements, 1 indicates\n",
      "        attention and 0 indicates no attention. Broadcasting can happen for\n",
      "        the missing batch dimensions and the head dimension.\n",
      "    return_attention_scores: A boolean to indicate whether the output should\n",
      "        be `(attention_output, attention_scores)` if `True`, or\n",
      "        `attention_output` if `False`. Defaults to `False`.\n",
      "    training: Python boolean indicating whether the layer should behave in\n",
      "        training mode (adding dropout) or in inference mode (no dropout).\n",
      "        Will go with either using the training mode of the parent\n",
      "        layer/model, or `False` (inference) if there is no parent layer.\n",
      "    use_causal_mask: A boolean to indicate whether to apply a causal mask to\n",
      "        prevent tokens from attending to future tokens (e.g., used in a\n",
      "        decoder Transformer).\n",
      "\n",
      "Returns:\n",
      "    attention_output: The result of the computation, of shape `(B, T, E)`,\n",
      "        where `T` is for target sequence shapes and `E` is the query input\n",
      "        last dimension if `output_shape` is `None`. Otherwise, the\n",
      "        multi-head outputs are projected to the shape specified by\n",
      "        `output_shape`.\n",
      "    attention_scores: (Optional) multi-head attention coefficients over\n",
      "        attention axes.\n",
      "\u001b[0;31mFile:\u001b[0m           /app/conda/envs/collegium/lib/python3.10/site-packages/keras/src/layers/attention/multi_head_attention.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "?MultiHeadAttention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collegium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
