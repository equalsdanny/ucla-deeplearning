{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "\n",
    "### Original Paper\n",
    "Dropout: A Simple Way to Prevent Neural Networks from Overfitting by\n",
    "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov (2014) http://jmlr.org/papers/v15/srivastava14a.html\n",
    "\n",
    "### Description\n",
    "At each iteration, we are going to disable randomly selected neurons, then do both forward and backward pass with those neurons disabled. Numerically, we can sample a vector from Bernoulli distribution,\n",
    "then multiply the neuron's activation matrix's with this vector column-wise (e.g. disabling the same neuron across all samples).\n",
    "\n",
    "### Mathematical Definition\n",
    "Let's define:\n",
    "* $ p = \\text{probability of keeping a node} $\n",
    "* $ \\mathbf{d} = \\text{vector of random variables ~ Bernoulli}(p)$\n",
    "\n",
    "Now we can zero-out randomly selected columns of the activation matrix:\n",
    "$$ \n",
    "\\tilde{\\mathbf{a}}_{:, m}^{[1]} \n",
    "= \\frac {\\mathbf{a}_{:, m}^{[1]} \\bigodot \\mathbf{d}_m} {p} \n",
    "$$\n",
    "\n",
    "Why divide by $p$? To keep the mean across the features constant:\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "    \\mathbb{E} [\\tilde{\\mathbf{a}}_{s, :}^{[1]}] \n",
    "    & = \\frac {\\mathbb{E}[\\mathbf{a}_{s, :}^{[1]}] \\cdot \\mathbb{E}[\\mathbf{d}]} {p} \\\\\n",
    "    & = \\frac {\\mathbb{E}[\\mathbf{a}_{s, :}^{[1]}] \\cdot p}{p} \\\\\n",
    "    & = \\mathbb{E}[\\mathbf{a}_{s, :}^{[1]}]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Regularization\n",
    "Why does this work as a regularization method? \n",
    "Intuitively, every node has to use information in all of its input nodes to minimize the impact of any particular input node being disabled. \n",
    "Numerically, this makes the L2-norm of the weights vector smaller.\n",
    "Theoretically, itâ€™s also similar to having an ensemble of neural networks, since each sampling of the dropout mask represents a different network.\n",
    "Overall, the dropout introduces noise robustness to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Applies Dropout to the input.\n",
       "\n",
       "The Dropout layer randomly sets input units to 0 with a frequency of `rate`\n",
       "at each step during training time, which helps prevent overfitting.\n",
       "Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over\n",
       "all inputs is unchanged.\n",
       "\n",
       "Note that the Dropout layer only applies when `training` is set to True\n",
       "such that no values are dropped during inference. When using `model.fit`,\n",
       "`training` will be appropriately set to True automatically, and in other\n",
       "contexts, you can set the kwarg explicitly to True when calling the layer.\n",
       "\n",
       "(This is in contrast to setting `trainable=False` for a Dropout layer.\n",
       "`trainable` does not affect the layer's behavior, as Dropout does\n",
       "not have any variables/weights that can be frozen during training.)\n",
       "\n",
       ">>> tf.random.set_seed(0)\n",
       ">>> layer = tf.keras.layers.Dropout(.2, input_shape=(2,))\n",
       ">>> data = np.arange(10).reshape(5, 2).astype(np.float32)\n",
       ">>> print(data)\n",
       "[[0. 1.]\n",
       " [2. 3.]\n",
       " [4. 5.]\n",
       " [6. 7.]\n",
       " [8. 9.]]\n",
       ">>> outputs = layer(data, training=True)\n",
       ">>> print(outputs)\n",
       "tf.Tensor(\n",
       "[[ 0.    1.25]\n",
       " [ 2.5   3.75]\n",
       " [ 5.    6.25]\n",
       " [ 7.5   8.75]\n",
       " [10.    0.  ]], shape=(5, 2), dtype=float32)\n",
       "\n",
       "Arguments:\n",
       "  rate: Float between 0 and 1. Fraction of the input units to drop.\n",
       "  noise_shape: 1D integer tensor representing the shape of the\n",
       "    binary dropout mask that will be multiplied with the input.\n",
       "    For instance, if your inputs have shape\n",
       "    `(batch_size, timesteps, features)` and\n",
       "    you want the dropout mask to be the same for all timesteps,\n",
       "    you can use `noise_shape=(batch_size, 1, features)`.\n",
       "  seed: A Python integer to use as random seed.\n",
       "\n",
       "Call arguments:\n",
       "  inputs: Input tensor (of any rank).\n",
       "  training: Python boolean indicating whether the layer should behave in\n",
       "    training mode (adding dropout) or in inference mode (doing nothing).\n",
       "\u001b[0;31mFile:\u001b[0m           /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     SpatialDropout1D, SpatialDropout2D, SpatialDropout3D, Dropout\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "?Dropout"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucla_deeplearning",
   "language": "python",
   "name": "ucla_deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
