{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Layer\n",
    "\n",
    "When there are several possible outcomes, expressing probability of each outcome is not possible with logistic activation, because activations between units must add up to one.\n",
    "\n",
    "$$ \\textbf{z}^{[l]} = \\text{exp}(\\textbf{a}^{[l-1]}) $$ \n",
    "\n",
    "$$ \n",
    "\\textbf{a}^{[l]}_{:, m} \n",
    "= \\frac\n",
    "    {\\textbf{z}^{[l]}_{:, m}}\n",
    "    {\\sum_{k=1}^M{\\textbf{z}^{[l]}_{:, k}}}\n",
    "$$\n",
    "\n",
    "Vector $\\textbf{z}^{[l]} \\in \\mathbb{R}^{S \\times M}$ is the result of element-wise exponentiation. When $M = 2$, the softmax activation is identical to the logistic activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a[l-1]</th>\n",
       "      <th>z[l]</th>\n",
       "      <th>a[l]</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2.72</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>7.39</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a[l-1]  z[l]  a[l]\n",
       "m                    \n",
       "0      -2  0.14  0.01\n",
       "1      -1  0.37  0.03\n",
       "2       0  1.00  0.09\n",
       "3       1  2.72  0.23\n",
       "4       2  7.39  0.64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['a[l-1]'] = [-2, -1, 0, 1, 2]\n",
    "df['z[l]'] = np.exp(df['a[l-1]'])\n",
    "df['a[l]'] = df['z[l]'] / df['z[l]'].sum()\n",
    "df = df.round(2)\n",
    "df.index.name = 'm'\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Softmax activation function.\n",
       "\n",
       "Input shape:\n",
       "  Arbitrary. Use the keyword argument `input_shape`\n",
       "  (tuple of integers, does not include the samples axis)\n",
       "  when using this layer as the first layer in a model.\n",
       "\n",
       "Output shape:\n",
       "  Same shape as the input.\n",
       "\n",
       "Arguments:\n",
       "  axis: Integer, axis along which the softmax normalization is applied.\n",
       "\u001b[0;31mFile:\u001b[0m           /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/advanced_activations.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Softmax\n",
    "?Softmax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collegium",
   "language": "python",
   "name": "collegium"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
